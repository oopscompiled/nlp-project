{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates steps of creating a pipeline of model training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- %pip install -q transformers datasets evaluate -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:06:53.693959Z",
     "iopub.status.busy": "2025-05-10T10:06:53.693697Z",
     "iopub.status.idle": "2025-05-10T10:07:00.232837Z",
     "shell.execute_reply": "2025-05-10T10:07:00.231951Z",
     "shell.execute_reply.started": "2025-05-10T10:06:53.693937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/oopscompiled/nlp-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:00.234142Z",
     "iopub.status.busy": "2025-05-10T10:07:00.233814Z",
     "iopub.status.idle": "2025-05-10T10:07:00.240196Z",
     "shell.execute_reply": "2025-05-10T10:07:00.239363Z",
     "shell.execute_reply.started": "2025-05-10T10:07:00.234108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd nlp-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:00.241317Z",
     "iopub.status.busy": "2025-05-10T10:07:00.241063Z",
     "iopub.status.idle": "2025-05-10T10:07:04.538766Z",
     "shell.execute_reply": "2025-05-10T10:07:04.537574Z",
     "shell.execute_reply.started": "2025-05-10T10:07:00.241292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%pip install -q transformers wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:04.540996Z",
     "iopub.status.busy": "2025-05-10T10:07:04.540764Z",
     "iopub.status.idle": "2025-05-10T10:07:08.382400Z",
     "shell.execute_reply": "2025-05-10T10:07:08.381511Z",
     "shell.execute_reply.started": "2025-05-10T10:07:04.540976Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import wandb\n",
    "import re\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "from transformers import AdamW\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# from datasets import Dataset\n",
    "\n",
    "from src.models import * # main models\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:08.384227Z",
     "iopub.status.busy": "2025-05-10T10:07:08.383768Z",
     "iopub.status.idle": "2025-05-10T10:07:08.438855Z",
     "shell.execute_reply": "2025-05-10T10:07:08.437708Z",
     "shell.execute_reply.started": "2025-05-10T10:07:08.384194Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "MAX_LEN = 64  # +2 for ([CLS], [SEP])\n",
    "DEVICE = torch.device('mps' if torch.backends.mps.is_available() else ('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "# VOCAB_SIZE = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:08.440044Z",
     "iopub.status.busy": "2025-05-10T10:07:08.439748Z",
     "iopub.status.idle": "2025-05-10T10:07:08.537199Z",
     "shell.execute_reply": "2025-05-10T10:07:08.536314Z",
     "shell.execute_reply.started": "2025-05-10T10:07:08.440024Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/working/nlp-project/data/train.csv')\n",
    "test = pd.read_csv('/kaggle/working/nlp-project/data/test.csv')\n",
    "validation = pd.read_csv('/kaggle/working/nlp-project/data/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:08.538609Z",
     "iopub.status.busy": "2025-05-10T10:07:08.538222Z",
     "iopub.status.idle": "2025-05-10T10:07:08.544237Z",
     "shell.execute_reply": "2025-05-10T10:07:08.543479Z",
     "shell.execute_reply.started": "2025-05-10T10:07:08.538546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_paraphrase_tokens(text):\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    tokens = ast.literal_eval(text)\n",
    "\n",
    "    cleaned_tokens = [token for token in tokens if not re.match(r'^(para|phrase).*', token.lower())]\n",
    "\n",
    "    full_text = \" \".join(cleaned_tokens)\n",
    "\n",
    "    for size in range(len(cleaned_tokens) // 2, 1, -1):\n",
    "        pattern = \" \".join(cleaned_tokens[:size])\n",
    "        if full_text.count(pattern) > 1:\n",
    "            full_text = full_text.replace(pattern + \" \" + pattern, pattern)\n",
    "    \n",
    "    final_tokens = full_text.split()\n",
    "\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:08.545307Z",
     "iopub.status.busy": "2025-05-10T10:07:08.545049Z",
     "iopub.status.idle": "2025-05-10T10:07:10.066518Z",
     "shell.execute_reply": "2025-05-10T10:07:10.065648Z",
     "shell.execute_reply.started": "2025-05-10T10:07:08.545275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_paraphrase_tokens)\n",
    "train['token_count'] = [len(sentence) for sentence in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.098264Z",
     "iopub.status.busy": "2025-05-10T10:07:10.098044Z",
     "iopub.status.idle": "2025-05-10T10:07:10.130058Z",
     "shell.execute_reply": "2025-05-10T10:07:10.129278Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.098243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossover(words1, words2):\n",
    "    if len(words1) < 3 or len(words2) < 3:\n",
    "        return [\" \".join(words1), \" \".join(words2)]\n",
    "    \n",
    "    split1 = len(words1) // 2\n",
    "    split2 = len(words2) // 2\n",
    "    \n",
    "    new_text1 = \" \".join(words1[split1:] + words2[split2:])\n",
    "    new_text2 = \" \".join(words2[:split2] + words1[split1:])\n",
    "    \n",
    "    return [new_text1, new_text2]\n",
    "\n",
    "labels_to_augment = ['sadness','fear', 'surprise']\n",
    "\n",
    "augmented_data = []\n",
    "\n",
    "max_rows = train['label'].value_counts(normalize=False).max()\n",
    "\n",
    "for label in labels_to_augment:\n",
    "\n",
    "    augment_df = train[train['label'] == label]\n",
    "    num_rows = len(augment_df)\n",
    "    rows_to_equality = max_rows - num_rows\n",
    "    num_pairs = max(int(num_rows * 0.14), rows_to_equality)\n",
    "\n",
    "    for row in range(num_pairs):\n",
    "        text1 = augment_df.iloc[row]['text']\n",
    "        text2 = augment_df.iloc[row + 1]['text']\n",
    "        new_texts = crossover(text1, text2)\n",
    "        for new_text in new_texts:\n",
    "            augmented_data.append({'label': label, 'text': new_text})\n",
    "\n",
    "augmented_words = pd.DataFrame(augmented_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.376508Z",
     "iopub.status.busy": "2025-05-10T10:07:10.376248Z",
     "iopub.status.idle": "2025-05-10T10:07:10.395950Z",
     "shell.execute_reply": "2025-05-10T10:07:10.395222Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.376458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmented_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.397022Z",
     "iopub.status.busy": "2025-05-10T10:07:10.396731Z",
     "iopub.status.idle": "2025-05-10T10:07:10.413177Z",
     "shell.execute_reply": "2025-05-10T10:07:10.412318Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.397002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train, augmented_words], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['token_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.414305Z",
     "iopub.status.busy": "2025-05-10T10:07:10.414030Z",
     "iopub.status.idle": "2025-05-10T10:07:10.438150Z",
     "shell.execute_reply": "2025-05-10T10:07:10.437319Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.414277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train['label'].value_counts(normalize=True).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.439131Z",
     "iopub.status.busy": "2025-05-10T10:07:10.438909Z",
     "iopub.status.idle": "2025-05-10T10:07:10.479142Z",
     "shell.execute_reply": "2025-05-10T10:07:10.478549Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.439102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "emotion_types = train['label'].unique().tolist()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_types)\n",
    "\n",
    "train['label'] = label_encoder.transform(train['label'])\n",
    "validation['label'] = label_encoder.transform(validation['label'])\n",
    "test['label'] = label_encoder.transform(test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.480219Z",
     "iopub.status.busy": "2025-05-10T10:07:10.479937Z",
     "iopub.status.idle": "2025-05-10T10:07:10.485903Z",
     "shell.execute_reply": "2025-05-10T10:07:10.485238Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.480193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=64):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.texts[index])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[index]\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.487036Z",
     "iopub.status.busy": "2025-05-10T10:07:10.486759Z",
     "iopub.status.idle": "2025-05-10T10:07:10.505026Z",
     "shell.execute_reply": "2025-05-10T10:07:10.504399Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.487007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:10.525207Z",
     "iopub.status.busy": "2025-05-10T10:07:10.524962Z",
     "iopub.status.idle": "2025-05-10T10:07:10.538181Z",
     "shell.execute_reply": "2025-05-10T10:07:10.537311Z",
     "shell.execute_reply.started": "2025-05-10T10:07:10.525178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Max len = {np.max(train['token_count'])}\\nMin len = {np.min(train['token_count'])}\\nAvg len = {np.round(np.mean(train['token_count']), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Токенизация\n",
    "RoBERTa: BPE-токенизатор лучше обрабатывает редкие слова и сложные языковые конструкции, что полезно для эмоциональных текстов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:38.782033Z",
     "iopub.status.busy": "2025-05-10T10:59:38.781735Z",
     "iopub.status.idle": "2025-05-10T10:59:39.885878Z",
     "shell.execute_reply": "2025-05-10T10:59:39.884646Z",
     "shell.execute_reply.started": "2025-05-10T10:59:38.782010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/deberta-base\"\n",
    "\n",
    "bert = AutoModel.from_pretrained(model_name).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:07:33.776403Z",
     "iopub.status.busy": "2025-05-10T10:07:33.774352Z",
     "iopub.status.idle": "2025-05-10T10:07:33.822726Z",
     "shell.execute_reply": "2025-05-10T10:07:33.821860Z",
     "shell.execute_reply.started": "2025-05-10T10:07:33.776367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = EmotionDataset(texts=train['text'].tolist(), labels=train['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "test_dataset = EmotionDataset(texts=test['text'].tolist(),labels=test['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "validation_dataset = EmotionDataset(texts=validation['text'].tolist(), labels=validation['label'].tolist(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularization, we employ two commonly used\n",
    "techniques: dropout (Hinton et al., 2012) and L2\n",
    "weight regularization. We apply dropout to prevent co-adaptation. In our model, we either apply\n",
    "dropout to word vectors before feeding the sequence\n",
    "of words into the convolutional layer or to the output\n",
    "of LSTM before the softmax layer. The L2 regularization is applied to the weight of the softmax layer. (https://arxiv.org/pdf/1511.08630)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:20.385113Z",
     "iopub.status.busy": "2025-05-10T10:59:20.384795Z",
     "iopub.status.idle": "2025-05-10T10:59:20.394881Z",
     "shell.execute_reply": "2025-05-10T10:59:20.394216Z",
     "shell.execute_reply.started": "2025-05-10T10:59:20.385087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(train.label), y=train.label)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:20.723334Z",
     "iopub.status.busy": "2025-05-10T10:59:20.723076Z",
     "iopub.status.idle": "2025-05-10T10:59:20.727168Z",
     "shell.execute_reply": "2025-05-10T10:59:20.726367Z",
     "shell.execute_reply.started": "2025-05-10T10:59:20.723314Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class_weights[3] *= 1.3\n",
    "class_weights[5] *= 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:21.153009Z",
     "iopub.status.busy": "2025-05-10T10:59:21.152711Z",
     "iopub.status.idle": "2025-05-10T10:59:21.159414Z",
     "shell.execute_reply": "2025-05-10T10:59:21.158636Z",
     "shell.execute_reply.started": "2025-05-10T10:59:21.152986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:22.441903Z",
     "iopub.status.busy": "2025-05-10T10:59:22.441626Z",
     "iopub.status.idle": "2025-05-10T10:59:22.458253Z",
     "shell.execute_reply": "2025-05-10T10:59:22.457483Z",
     "shell.execute_reply.started": "2025-05-10T10:59:22.441884Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "gru_model = MyGRU(embedding_dim=128, hidden_dim=128, output_dim=6, num_layers=2, dropout=0.4,fc_dropout=0.3, bidirectional=True, input_dropout=0.3).to(DEVICE)\n",
    "\n",
    "gru_optimizer =optim.Adam([\n",
    "    {'params': bert.parameters(), 'lr': 2e-5}, #2e-5\n",
    "    {'params': gru_model.parameters(), 'lr': 0.0002}\n",
    "], weight_decay=0.02)\n",
    "\n",
    "gru_scheduler = ReduceLROnPlateau(gru_optimizer, patience=5, factor=0.5, verbose=True)\n",
    "# gru_scheduler = CosineAnnealingWarmRestarts(gru_optimizer, T_0=5, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:23.716710Z",
     "iopub.status.busy": "2025-05-10T10:59:23.716370Z",
     "iopub.status.idle": "2025-05-10T10:59:23.749740Z",
     "shell.execute_reply": "2025-05-10T10:59:23.749113Z",
     "shell.execute_reply.started": "2025-05-10T10:59:23.716683Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lstm_model = MyLSTM(embedding_dim=128, hidden_dim=128, output_dim=6, num_layers=2, dropout=0.5, bidirectional=True,fc_dropout=0.3, input_dropout=0.2).to(DEVICE)\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=2e-4, weight_decay=0.0001) # lr=2e-5 may be optimal for bert \n",
    "lstm_scheduler = ReduceLROnPlateau(lstm_optimizer, patience=3, factor=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = HybridNN().to(DEVICE)\n",
    "hybrid_optimizer = AdamW([\n",
    "    {'params': bert.parameters(), 'lr': 1e-5},\n",
    "    {'params': hybrid_model.parameters(), 'lr': 0.00005}\n",
    "], weight_decay=0.05)\n",
    "\n",
    "# hybrid_scheduler = ReduceLROnPlateau(hybrid_optimizer, patience=3, factor=1e-2)\n",
    "hybrid_scheduler = CosineAnnealingWarmRestarts(hybrid_optimizer, T_0=3, T_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:59:26.294537Z",
     "iopub.status.busy": "2025-05-10T10:59:26.294248Z",
     "iopub.status.idle": "2025-05-10T10:59:26.298417Z",
     "shell.execute_reply": "2025-05-10T10:59:26.297399Z",
     "shell.execute_reply.started": "2025-05-10T10:59:26.294516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bert.config.dropout = 0.3\n",
    "\n",
    "# for m in bert.modules():\n",
    "#   for name, params in m.named_parameters():\n",
    "#     print(name, params.requires_grad)\n",
    "\n",
    "# freeze layers\n",
    "# for param in bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# unfreeze last 2 layers\n",
    "# for layer in bert.encoder.layer[-2:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-10T13:13:01.830Z",
     "iopub.execute_input": "2025-05-10T10:59:45.777311Z",
     "iopub.status.busy": "2025-05-10T10:59:45.777011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "MODEL = hybrid_model\n",
    "OPTIMIZER = hybrid_optimizer\n",
    "SCHEDULER = hybrid_scheduler\n",
    "\n",
    "lr_history = []\n",
    "early_stopper = EarlyStopper(patience=3, models=[bert, MODEL], min_delta=0.001, save_weights=False)\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    bert.train()\n",
    "    MODEL.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        OPTIMIZER.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = bert_output.last_hidden_state  # [batch_size, seq_len, 768]\n",
    "\n",
    "        outputs = MODEL(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    bert.eval()\n",
    "    MODEL.eval()\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    val_loss_total = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_loader:\n",
    "\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            embeddings = bert_output.last_hidden_state\n",
    "\n",
    "            outputs = MODEL(embeddings)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(validation_loader)\n",
    "    loss_history.append(avg_val_loss)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "    SCHEDULER.step(avg_val_loss)\n",
    "    current_lr = OPTIMIZER.param_groups[0]['lr']\n",
    "    lr_history.append(current_lr)\n",
    "\n",
    "    if early_stopper.early_stop(avg_val_loss):\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} — Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "class_report = classification_report(val_labels, val_preds, target_names=[str(i) for i in range(6)])\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:58:38.668309Z",
     "iopub.status.busy": "2025-05-10T10:58:38.668016Z",
     "iopub.status.idle": "2025-05-10T10:58:43.912694Z",
     "shell.execute_reply": "2025-05-10T10:58:43.911814Z",
     "shell.execute_reply.started": "2025-05-10T10:58:38.668289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if early_stopper.best_weights is not None:\n",
    "    bert.load_state_dict(early_stopper.best_weights[0])\n",
    "    MODEL.load_state_dict(early_stopper.best_weights[1])\n",
    "    print(\"Best weights loaded after training\")\n",
    "else:\n",
    "    print(\"No best weights were saved\")\n",
    "\n",
    "MODEL.eval()\n",
    "bert.eval()\n",
    "\n",
    "test_preds = []\n",
    "test_labels = []\n",
    "test_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "        bert_output = bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        embeddings = bert_output.last_hidden_state\n",
    "\n",
    "        outputs = MODEL(embeddings)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        test_loss_total += loss.item()\n",
    "\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "avg_test_loss = test_loss_total / len(test_loader)\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "\n",
    "print(f\"Validation Loss: {avg_test_loss:.4f} | Validation Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "class_report = classification_report(test_labels, test_preds, target_names=[str(i) for i in range(6)])\n",
    "\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(range(1, len(lr_history) + 1), lr_history)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Learning Rate Schedule\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "plt.plot(lr_history, loss_history, marker='o')\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss vs Learning Rate\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 605165,
     "sourceId": 1085454,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
