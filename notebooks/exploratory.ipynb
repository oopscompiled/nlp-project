{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we will look at exploratory data analysis (EDA) from which some insights can be drawn. The date has been augmented using google api for back translation and crossover tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/OopsWrongCode/nlp-project.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nlp-project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install regex nlpaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *\n",
    "\n",
    "PATH = loader()\n",
    "os.listdir(PATH)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_file_path = os.path.join(PATH,os.listdir(PATH)[0])\n",
    "test_file_path = os.path.join(PATH,os.listdir(PATH)[1])\n",
    "train_file_path = os.path.join(PATH,os.listdir(PATH)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file_path, sep=';', header=None)\n",
    "valid = pd.read_csv(val_file_path, sep=';', header=None)\n",
    "test = pd.read_csv(test_file_path, sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.rename({0 : 'text', 1 : 'label'}, inplace=True, axis=1)\n",
    "test.rename({0 : 'text', 1 : 'label'}, inplace=True, axis=1)\n",
    "valid.rename({0 : 'text', 1 : 'label'}, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of train : {train.shape}\\nShape of validation : {valid.shape}\\nShape of test : {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicates: {(len(train) - len(train.drop_duplicates(subset=['text'])))} ({round((len(train) - len(train.drop_duplicates(subset=['text']))) / len(train) * 100,2)}%)\")\n",
    "train = train.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of duplicates: {(len(valid) - len(valid.drop_duplicates(subset=['text'])))} ({round((len(valid) - len(valid.drop_duplicates(subset=['text']))) / len(valid) * 100,2)}%)\")\n",
    "valid = valid.drop_duplicates(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'].value_counts(normalize=True).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug.augmenter.word as naw\n",
    "\n",
    "# # https://nlpaug.readthedocs.io/en/latest/augmenter/word/context_word_embs.html\n",
    "\n",
    "# def compute_augment_size(label, train_data, max_augment=10, min_augment=0, base_multiplier=1.0):\n",
    "#     class_freq = train_data['label'].value_counts(normalize=True)\n",
    "    \n",
    "#     max_freq = class_freq.max()\n",
    "#     current_freq = class_freq[label]\n",
    "    \n",
    "#     augment_factor = max_freq / current_freq\n",
    "#     augment_size = min(int(augment_factor * base_multiplier), max_augment)\n",
    "    \n",
    "#     if current_freq > 0.8 * max_freq: # 0 for others\n",
    "#         augment_size = min_augment\n",
    "    \n",
    "#     return max(augment_size, min_augment)\n",
    "\n",
    "# def data_augmentation(text, label, train_data, max_augment=10, base_multiplier=1.0):\n",
    "#     augment_size = compute_augment_size(label, train_data, max_augment, min_augment=0, base_multiplier=base_multiplier)\n",
    "    \n",
    "#     aug = naw.ContextualWordEmbsAug(\n",
    "#         model_path='roberta-base',\n",
    "#         model_type='roberta',\n",
    "#         action='substitute',\n",
    "#         device=str(DEVICE),\n",
    "#         top_k=50,\n",
    "#         aug_max=5\n",
    "#     )\n",
    "    \n",
    "#     augmented_texts = []\n",
    "#     for _ in range(augment_size):\n",
    "#         aug_text = aug.augment(text)[0]\n",
    "#         augmented_texts.append(aug_text)\n",
    "    \n",
    "#     return augmented_texts\n",
    "\n",
    "# def augment_dataset(train_data, labels_to_augment=['anger', 'fear', 'love', 'surprise'], max_augment=10, base_multiplier=1.0):\n",
    "#     augmented_texts = []\n",
    "#     augmented_labels = []\n",
    "    \n",
    "#     for label in labels_to_augment:\n",
    "#         label_df = train_data[train_data['label'] == label]\n",
    "#         print(f\"Augmenting label: {label} ({len(label_df)} samples)\")\n",
    "        \n",
    "#         for _, row in label_df.iterrows():\n",
    "#             new_texts = data_augmentation(row['text'], label, train_data, max_augment, base_multiplier)\n",
    "#             augmented_texts.extend(new_texts)\n",
    "#             augmented_labels.extend([label] * len(new_texts))\n",
    "        \n",
    "#         print(f\"Label {label} augmented successfully! Added {len(label_df) * compute_augment_size(label, train_data, max_augment, base_multiplier=base_multiplier)} samples.\")\n",
    "    \n",
    "#     augmented_df = pd.DataFrame({\n",
    "#         'text': augmented_texts,\n",
    "#         'label': augmented_labels\n",
    "#     })\n",
    "    \n",
    "#     train_augmented = pd.concat([train_data, augmented_df], ignore_index=True)\n",
    "    \n",
    "#     print(f\"\\nRows after augmentation: {len(train_augmented)}\\n\")\n",
    "#     print(\"New class distribution:\")\n",
    "#     print(train_augmented['label'].value_counts(normalize=True))\n",
    "    \n",
    "#     return train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.kaggle.com/code/mujrush/data-augmentation-by-back-translation\n",
    "# %pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from googletrans import Translator\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # STEP 1: FIND APPROPRIATE SIZE OF AUGMENTATION\n",
    "# def compute_augment_size(label, train_data, max_augment=10, min_augment=0, base_multiplier=1.0): \n",
    "#     class_freq = train_data['label'].value_counts(normalize=True)\n",
    "#     max_freq = class_freq.max()\n",
    "#     current_freq = class_freq[label]\n",
    "    \n",
    "#     augment_factor = max_freq / current_freq\n",
    "#     augment_size = min(int(augment_factor * base_multiplier), max_augment)\n",
    "\n",
    "#     if current_freq > 0.8 * max_freq:\n",
    "#         augment_size = min_augment\n",
    "\n",
    "#     return max(augment_size, min_augment)\n",
    "\n",
    "# # STEP 2: MAIN FUNC\n",
    "# def back_translate_text(text, translator, src_lang='en', via_lang='fr', max_len=512):\n",
    "#     try:\n",
    "#         if len(text) > max_len:\n",
    "#             text = text[:max_len]\n",
    "\n",
    "#         via = translator.translate(text, dest=via_lang).text\n",
    "#         back = translator.translate(via, dest=src_lang).text\n",
    "#         return back\n",
    "#     except Exception as e:\n",
    "#         print(f\"Translation error: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # STEP 3: MAIN BODY\n",
    "# def back_translation(train_data, labels_to_augment=['anger', 'fear', 'love', 'surprise'], max_augment=10, base_multiplier=1.0):\n",
    "#     translator = Translator()\n",
    "#     augmented_texts = []\n",
    "#     augmented_labels = []\n",
    "\n",
    "#     for label in labels_to_augment:\n",
    "#         label_df = train_data[train_data['label'] == label]\n",
    "#         augment_size = compute_augment_size(label, train_data, max_augment, base_multiplier=base_multiplier)\n",
    "\n",
    "#         if augment_size == 0:\n",
    "#             continue\n",
    "\n",
    "#         n_to_augment = min(len(label_df), augment_size * len(label_df))\n",
    "#         label_subset = label_df.sample(n=n_to_augment, random_state=42)\n",
    "\n",
    "#         print(f\"Augmenting label: {label} ({len(label_df)} samples), will augment: {len(label_subset)}\")\n",
    "\n",
    "#         for _, row in tqdm(label_subset.iterrows(), total=len(label_subset), desc=f\"Back-translating {label}\"):\n",
    "#             orig_text = row['text']\n",
    "#             translated = back_translate_text(orig_text, translator)\n",
    "#             if translated:\n",
    "#                 augmented_texts.append(translated)\n",
    "#                 augmented_labels.append(label)\n",
    "\n",
    "#     augmented_df = pd.DataFrame({'text': augmented_texts, 'label': augmented_labels})\n",
    "#     train_augmented = pd.concat([train_data, augmented_df], ignore_index=True)\n",
    "#     train_augmented.drop_duplicates(subset=['text', 'label'], inplace=True)\n",
    "\n",
    "#     print(f\"\\nRows after augmentation: {len(train_augmented)}\\n\")\n",
    "#     print(\"New class distribution:\")\n",
    "#     print(train_augmented['label'].value_counts(normalize=True))\n",
    "\n",
    "#     return train_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = back_translation(train, max_augment=10, base_multiplier=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_augmented = augment_dataset(train, labels_to_augment=['anger', 'fear', 'love', 'surprise'], max_augment=10, base_multiplier=1.0)\n",
    "# train = train_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nlpaug.augmenter.word as naw \n",
    "\n",
    "# # https://nlpaug.readthedocs.io/en/latest/augmenter/word/context_word_embs.html\n",
    "\n",
    "# def compute_augment_size(label):\n",
    "#     class_counts = train['label'].value_counts(normalize=True)\n",
    "#     max_freq = class_counts.max()\n",
    "#     current_freq = class_counts[label]\n",
    "#     augment_size = int((max_freq - current_freq) * 100)\n",
    "    \n",
    "#     return max(augment_size, 1)\n",
    "\n",
    "\n",
    "# def data_augmentation(text, label):\n",
    "#     augment_size = compute_augment_size(label)\n",
    "#     aug = naw.ContextualWordEmbsAug(\n",
    "#         model_path='roberta-base',\n",
    "#         model_type='roberta',\n",
    "#         action='substitute',\n",
    "#         device=str(DEVICE),\n",
    "#         top_k=100,\n",
    "#         aug_max=augment_size\n",
    "#     )\n",
    "#     return aug.augment(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmented_texts = []\n",
    "# augmented_labels = []\n",
    "\n",
    "# LABELS = ['anger', 'fear', 'love', 'surprise']\n",
    "\n",
    "# for label in LABELS:\n",
    "#     label_df = train[train['label'] == label]\n",
    "\n",
    "#     for _, row in label_df.iterrows():\n",
    "#         new_texts = data_augmentation(row['text'], label)\n",
    "#         augmented_texts.extend(new_texts)\n",
    "#         augmented_labels.extend([label] * len(new_texts))\n",
    "#     print(f\"Label {label} augmented successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# augmented_df = pd.DataFrame({\n",
    "#     'text': augmented_texts,\n",
    "#     'label': augmented_labels\n",
    "# })\n",
    "\n",
    "# augmented_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.iloc[55]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import src.utils\n",
    "# print(src.utils.re.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(clean_for_bert)\n",
    "train['text'] = train['text'].apply(extract_clean_words)\n",
    "\n",
    "\n",
    "test['text'] = test['text'].apply(clean_for_bert)\n",
    "test['text'] = test['text'].apply(extract_clean_words)\n",
    "\n",
    "valid['text'] = valid['text'].apply(clean_for_bert)\n",
    "valid['text'] = valid['text'].apply(extract_clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "colors = ['grey','grey','grey','darkblue','darkblue','darkblue']\n",
    "train.groupby('label').text.count().sort_values().plot.barh(ylim=0, color=colors, title= 'COUNT OF EACH CATEGORIES')\n",
    "plt.xlabel('Number of Texts', fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Visualize the data\n",
    "# fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# train['label'].value_counts(sort=True).plot(kind='bar', color='mediumseagreen', fontsize = 16)\n",
    "# x = set(train['label'])\n",
    "\n",
    "# default_x_ticks = range(len(x))\n",
    "\n",
    "# plt.xticks(default_x_ticks, x, rotation=0, fontsize = 16)\n",
    "# plt.title('Target distribution', fontsize = 20)\n",
    "# plt.xlabel('Labels', fontsize = 20)\n",
    "# plt.ylabel('Number of MBTIs', fontsize = 20)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['token_count'] = [len(sentence) for sentence in train['text']]\n",
    "train['text_length'] = [len(seq) for seq in train['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.boxplot(x='label', y='token_count', data=train)\n",
    "plt.title('Tokens distribution', fontsize = 20)\n",
    "plt.xlabel('Labels', fontsize = 20)\n",
    "plt.ylabel('Number of emotion texts', fontsize = 20)\n",
    "plt.ylim((0,train['token_count'].max() + 10))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "sns.boxplot(x='label', y='text_length', data=train)\n",
    "plt.title('Length distribution', fontsize = 20)\n",
    "plt.xlabel('Labels', fontsize = 20)\n",
    "plt.ylabel('Length', fontsize = 20)\n",
    "plt.ylim((0,train['text_length'].max() + 10))\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embarked town distribution pie chart\n",
    "embark_counts = train['label'].value_counts()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.pie(embark_counts, labels=embark_counts.index, colors=sns.color_palette('Set3'), autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Pie chart of target variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw1 = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "from nltk.corpus import stopwords\n",
    "sw1 = set(sw1)\n",
    "sw = stopwords.words(\"english\")\n",
    "sw = set(sw)\n",
    "STOPWORDS = set.union(sw1, sw)\n",
    "\n",
    "# source https://github.com/6/stopwords-json/blob/master/dist/en.json\n",
    "\n",
    "# another stop words can be obtained here: spacy.load('en_core_web_sm') nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#noisy words\n",
    "UNWANTED_WORDS = set([\n",
    "    \"i\", \"it\", \"ur\", \"na\", \"omg\", \"people\", \"time\", \"good\", \"back\", \"gon\", \"day\", \"love\", \"happy\", \"lt\", \"kst\", 'im', 'feel', 'feeling', 'like', 'ive'\n",
    "])\n",
    "\n",
    "for l in train['label'].unique():\n",
    "\n",
    "    text_series = [\" \".join(text_list) for text_list in train[train.label == l]['text']]\n",
    "    label_text = \" \".join(text_series)\n",
    "\n",
    "    words = nltk.tokenize.word_tokenize(label_text)\n",
    "    \n",
    "    filtered_words = [\n",
    "        w for w in words if w.isalnum() and w not in STOPWORDS and w.lower() not in UNWANTED_WORDS\n",
    "    ]\n",
    "    \n",
    "    label_keywords = nltk.FreqDist(filtered_words)\n",
    "    \n",
    "    label_keywords_df = pd.DataFrame(label_keywords.items(), columns=['Word', 'Frequency'])\n",
    "    \n",
    "    label_keywords_df = label_keywords_df.sort_values(by='Frequency', ascending=False).head(15)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    sns.barplot(y=label_keywords_df['Word'], x=label_keywords_df['Frequency'], orient='h', ax=ax, palette=\"magma\")\n",
    "\n",
    "    ax.set_title(f'Top 15 keywords in {l} target tweets', fontsize=15)\n",
    "    ax.set_xlabel('Keyword Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "valid.to_csv('valid.csv', index=False)\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
